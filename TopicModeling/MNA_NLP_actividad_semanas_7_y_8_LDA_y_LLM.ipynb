{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hVND8xY2OKY"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestr√≠a en Inteligencia Artificial Aplicada\n",
        "#### Tecnol√≥gico de Monterrey\n",
        "#### Prof Luis Eduardo Falc√≥n Morales\n",
        "\n",
        "### **Adtividad en Equipos Semanas 7 y 8 : LDA y LMM audio-a-texto**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimHVFOv23lm"
      },
      "source": [
        "* **Nombres y matr√≠culas:**\n",
        "\n",
        "  *   Elemento de lista\n",
        "  *   Elemento de lista\n",
        "  *   Elemento de lista\n",
        "\n",
        "* **N√∫mero de Equipo:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jimvsiVgjMg"
      },
      "source": [
        "* ##### **En cada ejercicio pueden importar los paquetes o librer√≠as que requieran.**\n",
        "\n",
        "* ##### **En cada ejercicio pueden incluir las celdas y l√≠neas de c√≥digo que deseen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BtP-Sk0DT-M"
      },
      "source": [
        "# **Ejercicio 1:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh78pKeMghfe"
      },
      "source": [
        "* #### **Liga de los audios de las f√°bulas de Esopo:** https://www.gutenberg.org/ebooks/21144\n",
        "\n",
        "* #### **Descargar los 10 archivos de audio solicitados: 1, 4, 5, 6, 14, 22, 24, 25, 26, 27.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poQrSv5kdUoo",
        "outputId": "f2fb5eb6-e2b5-43fb-cbc6-7f365222b795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ek4SoVdtoyKe"
      },
      "outputs": [],
      "source": [
        "#Cargar archivos de audio\n",
        "audio_files = {\n",
        "    \"audio01\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-01.mp3\",\n",
        "    \"audio04\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-04.mp3\",\n",
        "    \"audio05\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-05.mp3\",\n",
        "    \"audio06\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-06.mp3\",\n",
        "    \"audio14\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-14.mp3\",\n",
        "    \"audio22\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-22.mp3\",\n",
        "    \"audio24\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-24.mp3\",\n",
        "    \"audio25\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-25.mp3\",\n",
        "    \"audio26\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-26.mp3\",\n",
        "    \"audio27\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-27.mp3\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK2HvUUrj-gi"
      },
      "source": [
        "Para convertir audio en texto, hay varias opciones con diferentes caracter√≠sticas y niveles de precisi√≥n.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1.   **Whisper de OpenAI en Hugging Face**\n",
        "\n",
        "*Versi√≥n gratuita y flexible con buena precisi√≥n.*\n",
        "*   Modelo de reconocimiento autom√°tico del habla (ASR) basado en un transformador codificador-decodificador.\n",
        "*   Puede transcribir y traducir audio en varios idiomas.\n",
        "* Disponible en diferentes tama√±os (tiny, base, small, medium, large) seg√∫n la velocidad y precisi√≥n requeridas.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2.  Whisper-1 de OpenAI\n",
        "\n",
        "*Mayor precisi√≥n y mejor manejo de ruido.*\n",
        "*   Versi√≥n optimizada de Whisper, utilizada en la API de OpenAI.\n",
        "*   Mayor precisi√≥n en transcripci√≥n y mejor manejo de ruido de fondo.\n",
        "*   Puede realizar transcripci√≥n y traducci√≥n en m√∫ltiples idiomas.\n",
        "*   Accesible a trav√©s de la API de OpenAI (costo)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3. Otros modelos en Hugging Face\n",
        "\n",
        "*Alternativas como wav2vec 2.0 pueden ser √∫tiles.*\n",
        "*   Existen m√∫ltiples modelos ASR en Hugging Face, como wav2vec 2.0, Conformer, y Whisper X.\n",
        "*   wav2vec 2.0 es eficiente en transcripci√≥n sin necesidad de alineaci√≥n previa.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "4.  Google Translator\n",
        "\n",
        "*preciso para espa√±ol, pero puede ser opci√≥n secundaria.*\n",
        "*   Google tiene modelos de reconocimiento de voz como Speech-to-Text API, pero no son tan avanzados como Whisper para transcripci√≥n en espa√±ol.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uYgtCvvJSmq"
      },
      "source": [
        "# **Ejercicio 2a:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAQjVP2HkoZY"
      },
      "source": [
        "* #### **Comenten el por qu√© del modelo seleccionado para extracci√≥n del texto de los audios.**\n",
        "\n",
        "* #### **Extraer el contenido de los audios en texto.**\n",
        "\n",
        "* #### **Sugerencia:** pueden extraerlo en un formato de diccionario, clave:valor $‚Üí$ {audio01:fabula01, ...}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3k5sLGhnO1d"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers accelerate librosa\n",
        "#!apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGq8P7SkHQv1"
      },
      "outputs": [],
      "source": [
        "#!pip install numpy==1.26.4 scipy==1.11.4 gensim==4.3.2 spacy==3.7.2 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o75FB9OuMJPn"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "# para filtrar advertencias:\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6KqUIaWm8ab",
        "outputId": "ee8fc104-b6e3-4955-f09b-933a143a98c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando audio01...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You have passed language=es, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of language=es.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando audio04...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando audio05...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando audio06...\n",
            "Procesando audio14...\n",
            "Procesando audio22...\n",
            "Procesando audio24...\n",
            "Procesando audio25...\n",
            "Procesando audio26...\n",
            "Procesando audio27...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Archivo: audio01\n",
            "Texto transcrito:\n",
            " Las F√°bulas de Esopo, grabado para LibriVox.org por Paulino, www.paulino.info. F√°bula n√∫mero 61. El Lobo y el Cordero en el Templo. D√°ndose cuenta de que era perseguido por un lobo, un peque√±o corderito decidi√≥ refugiarse en un templo cercano. Lo llam√≥ lobo y le dijo que si el sacrificador lo encontraba all√≠ adentro, lo enmolar√≠a a su dios. Mejor as√≠, replic√≥ el cordero, prefiero ser v√≠ctima para un dios a tener que perecer en tus colmillos. Si sin remedio vamos a ser sacrificados, m√°s nos vale que sea con el mayor honor. Fin de la f√°bula Esta es una grabaci√≥n del dominio p√∫blico.\n",
            "\n",
            "\n",
            "Archivo: audio04\n",
            "Texto transcrito:\n",
            " Las f√°bulas de Esopo grabado para LibriVox.org por Roberto Antonio Mu√±oz F√°bula n√∫mero 64 El lobo y la grulla A un lobo que com√≠a un hueso se le atragant√≥ el hueso en la garganta y corr√≠a por todas partes en busca de auxilio. Encontr√≥ en su correr a una grulla y le pidi√≥ que le salvara de aquella situaci√≥n y que enseguida le pagar√≠a por ello. Acept√≥ la grulla e introdujo su cabeza en la boca del lobo, sacando de la garganta el hueso atravesado Pidi√≥ entonces la cancelaci√≥n de la paga convenida Oye, Aniga, dijo el lobo, ¬øno crees que es suficiente paga con haber sacado tu cabeza sana y salva de mi boca? Nunca hagas favores a malvados, traficantes o corruptos, pues mucha paga tendr√≠as si te dejan sano y salvo Fin de f√°bula. Esta grabaci√≥n es de dominio p√∫blico.\n",
            "\n",
            "\n",
            "Archivo: audio05\n",
            "Texto transcrito:\n",
            " Las F√°bulas de Sopo, grabado para LibriVox.org por Karen Savage. F√°bula n√∫mero 65. El lobo y el caballo. Pasaba un lobo por un sembrado de cebada, pero como no era comida de su gusto, la dej√≥ y sigui√≥ su camino. Encontr√≥ al rato a un caballo y le llev√≥ al campo coment√°ndole la gran cantidad de cebada que hab√≠a hallado, pero que en vez de com√©rsela a √©l, mejor se la hab√≠a dejado porque le agradaba m√°s o√≠r el ruido de sus dientes al masticarla. Pero el caballo le repuso, amigo, si los lobos comieran cebada, no hubieras preferido complacer a tus o√≠dos sino a tu est√≥mago. A todo malvado, aunque parezca actuar como bueno, no debe de cre√©rsele. Fin de f√°bula. Esta grabaci√≥n es de dominio p√∫blico. Gracias por ver el video.\n",
            "\n",
            "\n",
            "Archivo: audio06\n",
            "Texto transcrito:\n",
            " Las f√°bulas de Esopo, grabado para LibriVox.org por Alejandro Gonz√°lez Calder√≥n. F√°bula n√∫mero 66, El lobo y el asno. Un lobo fue elegido rey entre sus cong√©neres y decret√≥ una ley ordenando que lo que cada uno capturase en la casa, lo pusiera en com√∫n y lo repartiese por partes iguales entre todos. de esta manera ya no tendr√≠an los lobos que devorarse unos a otros en √©pocas de hambre pero en eso le escuch√≥ un asno que estaba por ah√≠ cerca y moviendo sus orejas le dijo magn√≠fica idea ha brotado de tu coraz√≥n pero ¬øpor qu√© has escondido todo tu bot√≠n en tu cueva? llev√°lo a la comunidad y rep√°rtelo tambi√©n como lo has decretado el lobo descubierto y confundido derog√≥ su ley Si alguna vez llegas a tener poder de legislar, s√© el primero en cumplir tus propias leyes Fin de la f√°bula, esta grabaci√≥n es de dominio p√∫blico\n",
            "\n",
            "\n",
            "Archivo: audio14\n",
            "Texto transcrito:\n",
            " Las f√°bulas de Esopo. Grabado para LibriVox.org por El Ochito. F√°bula n√∫mero 74. El lobo y el cabrito encerrado. Protegido por la seguridad del corral de una casa, un cabrito vio pasar a un lobo y comenz√≥ a insultarle burl√°ndose ampliamente de √©l. El lobo serenamente le replic√≥. Infeliz, s√© que no eres t√∫ quien me est√° insultando sino el sitio en que te encuentras Muy a menudo no es el valor sino la ocasi√≥n y el lugar quienes proveen el enfrentamiento arrogante ante los poderosos Fin de la f√°bula Esta grabaci√≥n es del dominio p√∫blico Gracias.\n",
            "\n",
            "\n",
            "Archivo: audio22\n",
            "Texto transcrito:\n",
            " Las f√°bulas de Esopo. Grabado para LibriVox.org por El Ochito. F√°bula n√∫mero 82. El perro y la almeja. Un perro de esos acostumbrados a comer huevos, al ver una almeja, no lo pens√≥ dos veces y, creyendo que se trataba de un huevo, se la trag√≥ inmediatamente. Desgarradas luego sus entra√±as, se sinti√≥ muy mal y se dijo, bien merecido lo tengo, por creer que todo lo que veo redondo son huevos. Nunca tomes un asunto sin antes reflexionar, para no entrar luego en extra√±as dificultades. Fin de la f√°bula. Esta grabaci√≥n es del dominio p√∫blico.\n",
            "\n",
            "\n",
            "Archivo: audio24\n",
            "Texto transcrito:\n",
            " Las f√°bulas de Sopo. Grabado para LibriVox.org por Karen Savage. F√°bula n√∫mero 84. El perro y el reflejo en el r√≠o. Badiaba un perro un r√≠o, llevando en su hocico un sabroso pedazo de carne. Vio su propio reflejo en el agua del r√≠o, y crey√≥ que aquel reflejo era en realidad otro perro que llevaba un trozo de carne mayor que el suyo. Y deseando adue√±arse del pedazo ajeno, solt√≥ el suyo para arrebatar el trozo a su supuesto compadre, pero el resultado fue que se qued√≥ sin el propio y sin el ajeno. Este, porque no exist√≠a, s√≥lo era un reflejo, y el otro, el verdadero, porque se lo llev√≥ a la corriente. Nunca codicies el bien ajeno, pues puedes perder lo que ya has adquirido con tu esfuerzo. Fin de f√°bula Esta grabaci√≥n es de dominio p√∫blico\n",
            "\n",
            "\n",
            "Archivo: audio25\n",
            "Texto transcrito:\n",
            " Las f√°bulas de Esopo, grabado para LibreVox.org, f√°bula n√∫mero 85, El perro y el carnicero. Penetr√≥ un perro en una carnicer√≠a y notando que el carnicero estaba muy ocupado con sus clientes, cogi√≥ un trozo de carne y se li√≥ corriendo. Se volvi√≥ el carnicero y vi√©ndole huir, y sin poder hacer nada, exclam√≥. Oye amigo, all√≠ donde te encuentre no dejar√© de mirarte. No esperes a que suceda un accidente para pensar en c√≥mo evitarlo. Fin de f√°bula Esta grabaci√≥n es de dominio p√∫blico. Gracias.\n",
            "\n",
            "\n",
            "Archivo: audio26\n",
            "Texto transcrito:\n",
            " Las F√°bulas de Esopo, grabado para LibriVox.org por El Ochito. F√°bula n√∫mero 86. El perro con campanilla. Hab√≠a un perro que acostumbraba a morder sin raz√≥n. Le puso su amo una campanilla para advertirle a la gente de su presencia cercana, y el can, sonando la campanilla, se fue a la plaza p√∫blica a presumir. Mas una sabia perra, ya avanzada de a√±os, le dijo ¬øDe qu√© presumes tanto, amigo? S√© que no llevas esa campanilla por tus grandes virtudes, sino para anunciar tu maldad oculta Los halagos que se hacen a s√≠ mismo, los fanfarrones, s√≥lo delatan sus mayores defectos Fin de la f√°bula Esta grabaci√≥n es del dominio p√∫blico Gracias por ver el video.\n",
            "\n",
            "\n",
            "Archivo: audio27\n",
            "Texto transcrito:\n",
            " Las f√°bulas de Esopo. Grabado para LibriVox.org por El Ochito. F√°bula n√∫mero 87. El perro que persegu√≠a al le√≥n. Un perro de casa se encontr√≥ con un le√≥n y parti√≥ en su persecuci√≥n. Pero el le√≥n se volvi√≥ rugiendo, y el perro, todo atemorizado, retrocedi√≥ r√°pidamente por el mismo camino. Le vio una zorra y le dijo, ¬°Perro infeliz! Primero persegu√≠as al le√≥n y ya ni siquiera soportas sus surgidos. Cuando entres a una empresa, mantente siempre listo a afrontar imprevistos que no te imaginabas. Fin de la f√°bula. Esta grabaci√≥n es del dominio p√∫blico. Gracias.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#importar librer√≠as y configurar CUDA\n",
        "import torch\n",
        "from transformers import pipeline, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "import librosa\n",
        "\n",
        "# Configurar dispositivo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "\n",
        "#Modelo Whisper en Hugging face\n",
        "model_id = \"openai/whisper-large-v3-turbo\"\n",
        "\n",
        "# Cargar modelo y procesador\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "modelo = AutoModelForSpeechSeq2Seq.from_pretrained(model_id).to(device)\n",
        "\n",
        "# Inicializar pipeline\n",
        "pipe = pipeline(\"automatic-speech-recognition\",\n",
        "                model=modelo,\n",
        "                tokenizer=processor.tokenizer,\n",
        "                feature_extractor=processor.feature_extractor,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device=device)\n",
        "\n",
        "#Transcribir el audio y guardar los resultados\n",
        "# Diccionario para almacenar transcripciones\n",
        "transcripciones = {}\n",
        "\n",
        "# Procesar cada archivo de audio\n",
        "for key, audio_path in audio_files.items():\n",
        "    print(f\"Procesando {key}...\")\n",
        "    result = pipe(audio_path,\n",
        "                  return_timestamps=True,   # Para audios mayores a 30 segs.\n",
        "                  generate_kwargs={\"language\": \"es\"})\n",
        "    transcripciones[key] = result[\"text\"]\n",
        "\n",
        "# Mostrar los primeros resultados\n",
        "for key, value in transcripciones.items():\n",
        "    print(f\"\\nArchivo: {key}\")\n",
        "    print(f\"Texto transcrito:\\n{value}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM0D83j8EWiN"
      },
      "source": [
        "# **Ejercicio 2b:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiFG5q88EYHU"
      },
      "source": [
        "* #### **Eliminar el inicio y final comunes de los textos extra√≠dos de cada f√°bula.**\n",
        "\n",
        "* #### **Sugerencia:** Pueden guardar esta informaci√≥n en un archivo tipo JSON, para que al estar probando diferentes opciones en los ejercicios siguientes, puedan recuperar r√°pidamente la informaci√≥n de cada video/f√°bula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkbeTmeon_RP",
        "outputId": "e2363326-e7c1-44f0-def4-72d64578e3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ **Antes (audio01):**\n",
            " Las F√°bulas de Esopo, grabado para LibriVox.org por Paulino, www.paulino.info. F√°bula n√∫mero 61. El Lobo y el Cordero en el Templo. D√°ndose cuenta de que era perseguido por un lobo, un peque√±o corderito decidi√≥ refugiarse en un templo cercano. Lo llam√≥ lobo y le dijo que si el sacrificador lo encontraba all√≠ adentro, lo enmolar√≠a a su dios. Mejor as√≠, replic√≥ el cordero, prefiero ser v√≠ctima para un dios a tener que perecer en tus colmillos. Si sin remedio vamos a ser sacrificados, m√°s nos vale que sea con el mayor honor. Fin de la f√°bula Esta es una grabaci√≥n del dominio p√∫blico.\n",
            "\n",
            "‚úÖ **Despu√©s (audio01):**\n",
            "El Lobo y el Cordero en el Templo. D√°ndose cuenta de que era perseguido por un lobo, un peque√±o corderito decidi√≥ refugiarse en un templo cercano. Lo llam√≥ lobo y le dijo que si el sacrificador lo encontraba all√≠ adentro, lo enmolar√≠a a su dios. Mejor as√≠, replic√≥ el cordero, prefiero ser v√≠ctima para un dios a tener que perecer en tus colmillos. Si sin remedio vamos a ser sacrificados, m√°s nos vale que sea con el mayor honor.\n",
            "\n",
            "üéØ **Transcripciones limpias guardadas en /content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_recortadas.json**\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Expresi√≥n regular mejorada para eliminar todo lo anterior al n√∫mero de la f√°bula\n",
        "inicio_regex = r\"^.*?[Ff]√°bula n√∫mero \\d+[.,]?\\s*\"\n",
        "\n",
        "# Expresi√≥n regular para eliminar desde \"Fin de la f√°bula\" hasta el final\n",
        "final_regex = r\"\\s*Fin de la f√°bula.*?$\"\n",
        "\n",
        "# Diccionario para almacenar transcripciones limpias\n",
        "transcripciones_recortadas = {}\n",
        "\n",
        "# Limpiar cada transcripci√≥n eliminando el inicio y el final\n",
        "for key, texto in transcripciones.items():\n",
        "    texto_limpio = re.sub(inicio_regex, \"\", texto)  # Eliminar inicio\n",
        "    texto_limpio = re.sub(final_regex, \"\", texto_limpio)  # Eliminar final\n",
        "    transcripciones_recortadas[key] = texto_limpio.strip()\n",
        "\n",
        "# Mostrar el antes y despu√©s de la primera transcripci√≥n como ejemplo\n",
        "primer_audio = list(transcripciones.keys())[0]  # Obtener primera clave de audio\n",
        "print(f\"\\nüîπ **Antes ({primer_audio}):**\\n{transcripciones[primer_audio]}\")\n",
        "print(f\"\\n‚úÖ **Despu√©s ({primer_audio}):**\\n{transcripciones_recortadas[primer_audio]}\")\n",
        "\n",
        "# Guardar transcripciones limpias en un archivo JSON\n",
        "archivo_json = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_recortadas.json\"\n",
        "with open(archivo_json, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(transcripciones_recortadas, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"\\nüéØ **Transcripciones limpias guardadas en {archivo_json}**\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PKaB_Ge0Shc"
      },
      "source": [
        "# **Ejercicio 3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNrqcQFe0VWR"
      },
      "source": [
        "* #### **Apliquen el proceso de limpieza que consideren adecuado.**\n",
        "\n",
        "* #### **Justifiquen los pasos de limpieza utilizados. Tomen en cuenta que el texto extra√≠do de cada f√°bula es relativamente peque√±o.**\n",
        "\n",
        "* #### **En caso de que decidan no aplicar esta etapa de limpieza, deber√°n justificarlo.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS91CHdJ2nD2"
      },
      "source": [
        "Tomando en cuenta que el texto de las f√°bulas es corto, se busca un equilibrio entre limpieza y preservaci√≥n del contenido esencial.\n",
        "Se va a considerar:\n",
        "\n",
        "\n",
        "*   Eliminar caracteres especiales innecesarios para mejorar la legibilidad y evitar ruido en an√°lisis posteriores.\n",
        "*   Eliminar espacios dobles y saltos de l√≠nea para evitar problemas en el procesamiento y mejorar la estructura del texto.\n",
        "*   Eliminar algunas stopwords, como las f√°bulas tienen un lenguaje narrativo corto quitar algunas stopwords como \"pero\", \"porque\" o \"si\" podr√≠a afectar el significado.Solo se quitaran stopwords que no alteren la narrativa como \"el\", \"la\", etc.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "No se eliminar√°n:\n",
        "\n",
        "\n",
        "*   Palabras con baja frecuencia porque al ser un texto corto, muchas palabras aparecer√°n pocas veces y eliminarlas podr√≠a afectar el mensaje (ej: animales principales de la f√°bula).\n",
        "*   Palabras con longitud muy corta porque algunas afectar√≠an el sentido de frases clave (ej: si, no).\n",
        "*  Toda la puntuaci√≥n porque en textos narrativos se perder√≠a la estructura y claridad si se eliminan comas y puntos.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "pqwiCCdpq8D_",
        "outputId": "36a345bb-2a90-43f9-e677-03444bc08f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'transcripciones_recortadas' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3f6c505f2d60>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Diccionario de transcripciones limpias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtranscripciones_limpias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlimpiar_texto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexto\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranscripciones_recortadas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Mostrar antes y despu√©s de la primera transcripci√≥n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transcripciones_recortadas' is not defined"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar lista de stopwords en espa√±ol\n",
        "nltk.download('stopwords')\n",
        "stop_words_default = set(stopwords.words('spanish'))\n",
        "\n",
        "# Lista personalizada de stopwords que s√≠ eliminaremos (filtradas seg√∫n contexto)\n",
        "stop_words_filtradas = {\n",
        "    \"de\", \"la\", \"las\", \"los\", \"el\", \"un\", \"una\", \"unos\", \"unas\",\n",
        "    \"en\", \"es\", \"al\", \"del\", \"con\", \"por\", \"para\", \"se\", \"que\"\n",
        "}\n",
        "\n",
        "# Expresi√≥n regular para eliminar caracteres especiales innecesarios\n",
        "special_chars_regex = r\"[^a-zA-Z√°√©√≠√≥√∫√º√±√Å√â√ç√ì√ö√ú√ë0-9\\s]\"\n",
        "\n",
        "# Funci√≥n para limpiar cada texto\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()  # Convertir a min√∫sculas\n",
        "    texto = re.sub(special_chars_regex, \"\", texto)  # Eliminar caracteres especiales\n",
        "    texto = re.sub(r\"\\s+\", \" \", texto).strip()  # Eliminar espacios dobles y saltos de l√≠nea\n",
        "\n",
        "    # Eliminar solo las stopwords de nuestra lista personalizada\n",
        "    palabras = texto.split()\n",
        "    texto_limpio = \" \".join([palabra for palabra in palabras if palabra not in stop_words_filtradas])\n",
        "\n",
        "    return texto_limpio\n",
        "\n",
        "# Diccionario de transcripciones limpias\n",
        "transcripciones_limpias = {key: limpiar_texto(texto) for key, texto in transcripciones_recortadas.items()}\n",
        "\n",
        "# Mostrar antes y despu√©s de la primera transcripci√≥n\n",
        "primer_audio = list(transcripciones_recortadas.keys())[0]\n",
        "print(f\"\\nüîπ **Antes ({primer_audio}):**\\n{transcripciones_recortadas[primer_audio]}\")\n",
        "print(f\"\\n‚úÖ **Despu√©s ({primer_audio}):**\\n{transcripciones_limpias[primer_audio]}\")\n",
        "\n",
        "# Guardar transcripciones limpias en JSON\n",
        "archivo_json = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_limpias.json\"\n",
        "with open(archivo_json, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(transcripciones_limpias, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"\\nüéØ **Transcripciones limpias guardadas en {archivo_json}**\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2ywrmsMP_EF"
      },
      "source": [
        "# **Ejercicio 4:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xFpnt0A0Ub7",
        "outputId": "fd6919ca-b53a-45b6-cbf6-50ad607603fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ **Pesos de palabras guardados en /content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/palabras_con_pesos.json**\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "import nltk\n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Descargar recursos de NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "# üìå **1. Cargar las f√°bulas limpias**\n",
        "archivo_json = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_limpias.json\"\n",
        "with open(archivo_json, \"r\", encoding=\"utf-8\") as f:\n",
        "    fabulas_limpias = json.load(f)\n",
        "\n",
        "# üìå **2. Aplicar LDA individualmente a cada f√°bula**\n",
        "num_palabras = 20  # N√∫mero de palabras clave por f√°bula\n",
        "pesos_palabras_fabulas = {}\n",
        "\n",
        "for key, texto in fabulas_limpias.items():\n",
        "    # Tokenizar la f√°bula individualmente\n",
        "    tokens_fabula = word_tokenize(texto)\n",
        "\n",
        "    # Crear diccionario y corpus solo para esta f√°bula\n",
        "    diccionario = Dictionary([tokens_fabula])\n",
        "    corpus = [diccionario.doc2bow(tokens_fabula)]\n",
        "\n",
        "    # Aplicar LDA SOLO a la f√°bula actual\n",
        "    modelo_lda = LdaModel(corpus, num_topics=1, id2word=diccionario, passes=10)\n",
        "\n",
        "    # Extraer palabras clave y sus pesos del √∫nico t√≥pico generado\n",
        "    palabras_con_pesos = modelo_lda.show_topic(0, topn=num_palabras)\n",
        "\n",
        "    # Convertir pesos a `float` para evitar errores de serializaci√≥n\n",
        "    pesos_palabras_fabulas[key] = {palabra: float(peso) for palabra, peso in palabras_con_pesos}\n",
        "\n",
        "# üìå **3. Guardar resultados en JSON**\n",
        "archivo_salida = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/palabras_con_pesos.json\"\n",
        "with open(archivo_salida, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(pesos_palabras_fabulas, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"\\n‚úÖ **Pesos de palabras guardados en {archivo_salida}**\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blrrs1sWwkSx"
      },
      "source": [
        "# **Ejercicio 5a y 5b:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWvzQ-aNwsVk"
      },
      "source": [
        "* #### **5a: Mediante el LLM que hayan seleccionado, generar un √∫nico enunciado que describa o resuma cada f√°bula.**\n",
        "\n",
        "* #### **5b: Mediante el LLM que hayan seleccionado, generar tres posibles enunciados diferentes relacionados con la historia de la f√°bula.**\n",
        "\n",
        "* #### **Sugerencia:** En realidad los dos incisos a y b se pueden obtener con un solo prompt que solicite la informaci√≥n y el formato correspondiente para cada una de estas partes. Por ejemplo, para cada f√°bula la salida puede ser un primer enunciado gen√©rico que resume o describe dicha tem√°tica; seguido de tres enunciados, cada uno hablando sobre una situaci√≥n o parte diferente de la f√°bula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BXeGlgwEWGvc"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import gc\n",
        "from transformers import pipeline\n",
        "\n",
        "# üîπ Liberar memoria antes de la ejecuci√≥n (Optimizaci√≥n en Colab)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# üìå **1. Cargar palabras clave y sus pesos extra√≠das con LDA**\n",
        "archivo_json = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/palabras_con_pesos.json\"\n",
        "with open(archivo_json, \"r\", encoding=\"utf-8\") as f:\n",
        "    pesos_palabras_fabulas = json.load(f)\n",
        "\n",
        "# üìå **2. Selecci√≥n del modelo LLM en Hugging Face**\n",
        "modelo_huggingface = \"microsoft/Phi-2\"  # Modelo ligero y eficiente\n",
        "llm_pipeline = pipeline(\"text-generation\", model=modelo_huggingface, torch_dtype=torch.float16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# üìå **3. Generaci√≥n de res√∫menes y subtemas**\n",
        "resultados = {}\n",
        "\n",
        "for key, palabras_con_pesos in pesos_palabras_fabulas.items():\n",
        "    # **Organizar palabras seg√∫n su peso**\n",
        "    palabras_ordenadas = sorted(palabras_con_pesos.items(), key=lambda x: x[1], reverse=True)\n",
        "    palabras_clave = [palabra for palabra, peso in palabras_ordenadas]\n",
        "\n",
        "    # üìå **4. Mejorar el prompt para el resumen**\n",
        "    prompt_resumen = f\"\"\"\n",
        "Eres un narrador experto en literatura y f√°bulas.\n",
        "Tu tarea es escribir un **√∫nico enunciado** que resuma la historia basada en las siguientes palabras clave.\n",
        "Las palabras m√°s importantes est√°n al inicio y deben ser el centro de la historia.\n",
        "Evita definiciones, explicaciones o contenido ajeno a la f√°bula.\n",
        "Escribe una historia clara, directa y con sentido.\n",
        "\n",
        "Ejemplo de salida:\n",
        "üìñ *El lobo persigui√≥ al cordero hasta el templo, pero fue detenido por los dioses, que castigaron su ferocidad.*\n",
        "\n",
        "Ahora genera un resumen basado en estas palabras clave:\n",
        "\n",
        "Palabras clave con peso: {', '.join(palabras_clave)}\n",
        "\n",
        "Resumen:\n",
        "\"\"\"\n",
        "\n",
        "    # üìå **5. Mejorar el prompt para los subtemas**\n",
        "    prompt_subtemas = f\"\"\"\n",
        "Usando las siguientes palabras clave, escribe **tres enunciados** diferentes que se relacionen con la f√°bula.\n",
        "Cada enunciado debe explorar un aspecto distinto de la historia.\n",
        "No repitas el resumen, cada subtema debe agregar una perspectiva nueva.\n",
        "\n",
        "Ejemplo de salida:\n",
        "üîπ *La justicia divina castiga a quienes act√∫an con maldad.*\n",
        "üîπ *El lobo representa la agresividad y el peligro de dejarse llevar por los instintos.*\n",
        "üîπ *El cordero simboliza la inocencia y la vulnerabilidad en un mundo cruel.*\n",
        "\n",
        "Ahora genera tres enunciados basados en estas palabras clave:\n",
        "\n",
        "Palabras clave con peso: {', '.join(palabras_clave)}\n",
        "\n",
        "Subtemas:\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "\"\"\"\n",
        "\n",
        "    # üìå **6. Generar el resumen y los subtemas con Hugging Face LLM**\n",
        "    respuesta_resumen = llm_pipeline(prompt_resumen, max_length=50, do_sample=True, temperature=0.8)[0][\"generated_text\"].replace(prompt_resumen, \"\").strip()\n",
        "    respuesta_subtemas = llm_pipeline(prompt_subtemas, max_length=100, do_sample=True, temperature=0.8)[0][\"generated_text\"].replace(prompt_subtemas, \"\").strip().split(\"\\n\")\n",
        "\n",
        "    # üìå **7. Filtrar respuestas irrelevantes y asegurar estructura**\n",
        "    respuesta_subtemas = [s.strip() for s in respuesta_subtemas if len(s.strip()) > 5 and not s.strip().isdigit()]\n",
        "    while len(respuesta_subtemas) < 3:\n",
        "        respuesta_subtemas.append(\"N/A\")  # Evita listas incompletas\n",
        "\n",
        "    # üìå **8. Guardar resultados**\n",
        "    resultados[key] = {\n",
        "        \"resumen\": respuesta_resumen,\n",
        "        \"subtemas\": respuesta_subtemas[:3]  # Limita a 3 subtemas\n",
        "    }\n",
        "\n",
        "# üìå **9. Guardar resultados en JSON**\n",
        "archivo_salida = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_resumen_subtemas.json\"\n",
        "with open(archivo_salida, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(resultados, f, ensure_ascii=False, indent=4, sort_keys=True)\n",
        "\n",
        "print(f\"\\n‚úÖ **Res√∫menes y subtemas generados y guardados en {archivo_salida}**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868,
          "referenced_widgets": [
            "4a5a6e0c5dda4b4e954a03548f8501c8",
            "d118fb471f934598bee403f3fa3d01d9",
            "5691e23557f34a7eb2597fd012539d01",
            "e00518f0e4cb4a5a9e9bf06c0ed15e98",
            "06d5977410de41cba0acfd85ee5e16f4",
            "bdee4d7eb2454b07a8092d02e3b420f4",
            "b79efc35283c4155a5d2f87b148e8c6b",
            "08c6a87d2a614d63bee11cfc14231bef",
            "20d908d2b690478ba82b3840bac960e1",
            "bbfa573d052b428ba7ba9505f5c1ce98",
            "a01f66c2f0814f54b33b9a2b5739968a"
          ]
        },
        "id": "7s7tzj2qxrA7",
        "outputId": "ad4f04b5-bc4e-4c3d-8f2f-219162f97370"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a5a6e0c5dda4b4e954a03548f8501c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ **Res√∫menes y subtemas generados y guardados en /content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_resumen_subtemas.json**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868,
          "referenced_widgets": [
            "a26b594632b04457a6daf8d153a4b5d8",
            "8c079b5e63ee4bda9fdffe76556def84",
            "da13d23cf9154e9dbab742f56bfcd0fb",
            "a758f7fa52034ca0acaec04f59a7ff89",
            "35b9523adb3a4f66b12c464158164a4f",
            "ab8d0956791d460e845f1593675377fc",
            "dc77bbe4e5364974b09b76cf45b2d06b",
            "51a6dbef47b14aadb49ef39520a6d9b5",
            "f0e4f32da38a4407a6c7b8b09e826b16",
            "9150a30d43cb4f3787a182b601cbbe60",
            "d73cbb5da23447f6b9771758e2de858b"
          ]
        },
        "id": "Q9UkVPxM0Xii",
        "outputId": "4f749d1c-e67a-4b9f-b1f5-78d901a79278"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a26b594632b04457a6daf8d153a4b5d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ **Res√∫menes y subtemas guardados en /content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_resumen_subtemas.json**\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "# Cargar palabras clave de cada f√°bula\n",
        "archivo_json = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/palabras_clave.json\"\n",
        "with open(archivo_json, \"r\", encoding=\"utf-8\") as f:\n",
        "    palabras_clave_fabulas = json.load(f)\n",
        "\n",
        "# Selecci√≥n del modelo LLM en Hugging Face\n",
        "modelo_huggingface = \"microsoft/Phi-2\"  # Lightweight alternative\n",
        "#\"mistralai/Mistral-7B-Instruct\", \"meta-llama/Llama-3-8B\" , \"tiiuae/falcon-7b-instruct\", \"bigscience/bloomz-7b\"\n",
        "\n",
        "# Inicializar el pipeline de generaci√≥n de texto\n",
        "llm_pipeline = pipeline(\"text-generation\", model=modelo_huggingface, torch_dtype=torch.float16)\n",
        "\n",
        "# Generaci√≥n de res√∫menes y subtemas\n",
        "resultados = {}\n",
        "\n",
        "for key, palabras in palabras_clave_fabulas.items():\n",
        "    prompt_resumen = f\"Usando las siguientes palabras clave, genera un √∫nico enunciado que describa la f√°bula:\\nPalabras clave: {', '.join(palabras)}\"\n",
        "    prompt_subtemas = f\"Usando solo las siguientes palabras clave, genera cinco posibles subtemas diferentes para esta f√°bula.Responde √∫nicamente con una lista, sin introducciones ni explicaciones. Palabras clave: {', '.join(palabras)}\"\n",
        "\n",
        "    # Generaci√≥n con Hugging Face LLM\n",
        "    respuesta_resumen = llm_pipeline(prompt_resumen, max_length=30, do_sample=True)[0][\"generated_text\"]\n",
        "    respuesta_subtemas = llm_pipeline(prompt_subtemas, max_length=10, do_sample=True)[0][\"generated_text\"].split(\"\\n\")\n",
        "\n",
        "    # Guardar resultados\n",
        "    resultados[key] = {\n",
        "        \"resumen\": respuesta_resumen,\n",
        "        \"subtemas\": respuesta_subtemas\n",
        "    }\n",
        "\n",
        "# Guardar resultados en JSON\n",
        "archivo_salida = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_resumen_subtemas.json\"\n",
        "with open(archivo_salida, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(resultados, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"\\nüéØ **Res√∫menes y subtemas guardados en {archivo_salida}**\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-dZSFJz9cK"
      },
      "source": [
        "# **Ejercicio 6:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w3usdaC0BCj"
      },
      "source": [
        "* #### **Incluyan sus conclusiones de la actividad audio-a-texto:**\n",
        "\n",
        "\n",
        "\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtB5Q3m41YQ0"
      },
      "source": [
        "# **Fin de la actividad LDA y LMM: audio-a-texto**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a5a6e0c5dda4b4e954a03548f8501c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d118fb471f934598bee403f3fa3d01d9",
              "IPY_MODEL_5691e23557f34a7eb2597fd012539d01",
              "IPY_MODEL_e00518f0e4cb4a5a9e9bf06c0ed15e98"
            ],
            "layout": "IPY_MODEL_06d5977410de41cba0acfd85ee5e16f4"
          }
        },
        "d118fb471f934598bee403f3fa3d01d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdee4d7eb2454b07a8092d02e3b420f4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b79efc35283c4155a5d2f87b148e8c6b",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "5691e23557f34a7eb2597fd012539d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08c6a87d2a614d63bee11cfc14231bef",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20d908d2b690478ba82b3840bac960e1",
            "value": 2
          }
        },
        "e00518f0e4cb4a5a9e9bf06c0ed15e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbfa573d052b428ba7ba9505f5c1ce98",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a01f66c2f0814f54b33b9a2b5739968a",
            "value": "‚Äá2/2‚Äá[00:00&lt;00:00,‚Äá‚Äá2.38it/s]"
          }
        },
        "06d5977410de41cba0acfd85ee5e16f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdee4d7eb2454b07a8092d02e3b420f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b79efc35283c4155a5d2f87b148e8c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08c6a87d2a614d63bee11cfc14231bef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d908d2b690478ba82b3840bac960e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbfa573d052b428ba7ba9505f5c1ce98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a01f66c2f0814f54b33b9a2b5739968a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a26b594632b04457a6daf8d153a4b5d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c079b5e63ee4bda9fdffe76556def84",
              "IPY_MODEL_da13d23cf9154e9dbab742f56bfcd0fb",
              "IPY_MODEL_a758f7fa52034ca0acaec04f59a7ff89"
            ],
            "layout": "IPY_MODEL_35b9523adb3a4f66b12c464158164a4f"
          }
        },
        "8c079b5e63ee4bda9fdffe76556def84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab8d0956791d460e845f1593675377fc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dc77bbe4e5364974b09b76cf45b2d06b",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "da13d23cf9154e9dbab742f56bfcd0fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51a6dbef47b14aadb49ef39520a6d9b5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0e4f32da38a4407a6c7b8b09e826b16",
            "value": 2
          }
        },
        "a758f7fa52034ca0acaec04f59a7ff89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9150a30d43cb4f3787a182b601cbbe60",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d73cbb5da23447f6b9771758e2de858b",
            "value": "‚Äá2/2‚Äá[00:00&lt;00:00,‚Äá‚Äá1.29it/s]"
          }
        },
        "35b9523adb3a4f66b12c464158164a4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab8d0956791d460e845f1593675377fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc77bbe4e5364974b09b76cf45b2d06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51a6dbef47b14aadb49ef39520a6d9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e4f32da38a4407a6c7b8b09e826b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9150a30d43cb4f3787a182b601cbbe60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d73cbb5da23447f6b9771758e2de858b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}