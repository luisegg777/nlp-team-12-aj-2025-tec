{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hVND8xY2OKY"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestría en Inteligencia Artificial Aplicada\n",
        "#### Tecnológico de Monterrey\n",
        "#### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "### **Adtividad en Equipos Semanas 7 y 8 : LDA y LMM audio-a-texto**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimHVFOv23lm"
      },
      "source": [
        "* **Nombres y matrículas:**\n",
        "\n",
        "  *   Elemento de lista\n",
        "  *   Elemento de lista\n",
        "  *   Elemento de lista\n",
        "\n",
        "* **Número de Equipo:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jimvsiVgjMg"
      },
      "source": [
        "* ##### **En cada ejercicio pueden importar los paquetes o librerías que requieran.**\n",
        "\n",
        "* ##### **En cada ejercicio pueden incluir las celdas y líneas de código que deseen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BtP-Sk0DT-M"
      },
      "source": [
        "# **Ejercicio 1:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh78pKeMghfe"
      },
      "source": [
        "* #### **Liga de los audios de las fábulas de Esopo:** https://www.gutenberg.org/ebooks/21144\n",
        "\n",
        "* #### **Descargar los 10 archivos de audio solicitados: 1, 4, 5, 6, 14, 22, 24, 25, 26, 27.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poQrSv5kdUoo",
        "outputId": "f2fb5eb6-e2b5-43fb-cbc6-7f365222b795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ek4SoVdtoyKe"
      },
      "outputs": [],
      "source": [
        "#Cargar archivos de audio\n",
        "audio_files = {\n",
        "    \"audio01\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-01.mp3\",\n",
        "    \"audio04\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-04.mp3\",\n",
        "    \"audio05\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-05.mp3\",\n",
        "    \"audio06\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-06.mp3\",\n",
        "    \"audio14\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-14.mp3\",\n",
        "    \"audio22\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-22.mp3\",\n",
        "    \"audio24\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-24.mp3\",\n",
        "    \"audio25\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-25.mp3\",\n",
        "    \"audio26\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-26.mp3\",\n",
        "    \"audio27\": \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/21144-27.mp3\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK2HvUUrj-gi"
      },
      "source": [
        "Para convertir audio en texto, hay varias opciones con diferentes características y niveles de precisión.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1.   **Whisper de OpenAI en Hugging Face**\n",
        "\n",
        "*Versión gratuita y flexible con buena precisión.*\n",
        "*   Modelo de reconocimiento automático del habla (ASR) basado en un transformador codificador-decodificador.\n",
        "*   Puede transcribir y traducir audio en varios idiomas.\n",
        "* Disponible en diferentes tamaños (tiny, base, small, medium, large) según la velocidad y precisión requeridas.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2.  Whisper-1 de OpenAI\n",
        "\n",
        "*Mayor precisión y mejor manejo de ruido.*\n",
        "*   Versión optimizada de Whisper, utilizada en la API de OpenAI.\n",
        "*   Mayor precisión en transcripción y mejor manejo de ruido de fondo.\n",
        "*   Puede realizar transcripción y traducción en múltiples idiomas.\n",
        "*   Accesible a través de la API de OpenAI (costo)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3. Otros modelos en Hugging Face\n",
        "\n",
        "*Alternativas como wav2vec 2.0 pueden ser útiles.*\n",
        "*   Existen múltiples modelos ASR en Hugging Face, como wav2vec 2.0, Conformer, y Whisper X.\n",
        "*   wav2vec 2.0 es eficiente en transcripción sin necesidad de alineación previa.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "4.  Google Translator\n",
        "\n",
        "*preciso para español, pero puede ser opción secundaria.*\n",
        "*   Google tiene modelos de reconocimiento de voz como Speech-to-Text API, pero no son tan avanzados como Whisper para transcripción en español.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uYgtCvvJSmq"
      },
      "source": [
        "# **Ejercicio 2a:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAQjVP2HkoZY"
      },
      "source": [
        "* #### **Comenten el por qué del modelo seleccionado para extracción del texto de los audios.**\n",
        "\n",
        "* #### **Extraer el contenido de los audios en texto.**\n",
        "\n",
        "* #### **Sugerencia:** pueden extraerlo en un formato de diccionario, clave:valor $→$ {audio01:fabula01, ...}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3k5sLGhnO1d"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers accelerate librosa\n",
        "#!apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGq8P7SkHQv1"
      },
      "outputs": [],
      "source": [
        "#!pip install numpy==1.26.4 scipy==1.11.4 gensim==4.3.2 spacy==3.7.2 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o75FB9OuMJPn"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "# para filtrar advertencias:\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6KqUIaWm8ab",
        "outputId": "ee8fc104-b6e3-4955-f09b-933a143a98c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando audio01...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You have passed language=es, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of language=es.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando audio04...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando audio05...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando audio06...\n",
            "Procesando audio14...\n",
            "Procesando audio22...\n",
            "Procesando audio24...\n",
            "Procesando audio25...\n",
            "Procesando audio26...\n",
            "Procesando audio27...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Archivo: audio01\n",
            "Texto transcrito:\n",
            " Las Fábulas de Esopo, grabado para LibriVox.org por Paulino, www.paulino.info. Fábula número 61. El Lobo y el Cordero en el Templo. Dándose cuenta de que era perseguido por un lobo, un pequeño corderito decidió refugiarse en un templo cercano. Lo llamó lobo y le dijo que si el sacrificador lo encontraba allí adentro, lo enmolaría a su dios. Mejor así, replicó el cordero, prefiero ser víctima para un dios a tener que perecer en tus colmillos. Si sin remedio vamos a ser sacrificados, más nos vale que sea con el mayor honor. Fin de la fábula Esta es una grabación del dominio público.\n",
            "\n",
            "\n",
            "Archivo: audio04\n",
            "Texto transcrito:\n",
            " Las fábulas de Esopo grabado para LibriVox.org por Roberto Antonio Muñoz Fábula número 64 El lobo y la grulla A un lobo que comía un hueso se le atragantó el hueso en la garganta y corría por todas partes en busca de auxilio. Encontró en su correr a una grulla y le pidió que le salvara de aquella situación y que enseguida le pagaría por ello. Aceptó la grulla e introdujo su cabeza en la boca del lobo, sacando de la garganta el hueso atravesado Pidió entonces la cancelación de la paga convenida Oye, Aniga, dijo el lobo, ¿no crees que es suficiente paga con haber sacado tu cabeza sana y salva de mi boca? Nunca hagas favores a malvados, traficantes o corruptos, pues mucha paga tendrías si te dejan sano y salvo Fin de fábula. Esta grabación es de dominio público.\n",
            "\n",
            "\n",
            "Archivo: audio05\n",
            "Texto transcrito:\n",
            " Las Fábulas de Sopo, grabado para LibriVox.org por Karen Savage. Fábula número 65. El lobo y el caballo. Pasaba un lobo por un sembrado de cebada, pero como no era comida de su gusto, la dejó y siguió su camino. Encontró al rato a un caballo y le llevó al campo comentándole la gran cantidad de cebada que había hallado, pero que en vez de comérsela a él, mejor se la había dejado porque le agradaba más oír el ruido de sus dientes al masticarla. Pero el caballo le repuso, amigo, si los lobos comieran cebada, no hubieras preferido complacer a tus oídos sino a tu estómago. A todo malvado, aunque parezca actuar como bueno, no debe de creérsele. Fin de fábula. Esta grabación es de dominio público. Gracias por ver el video.\n",
            "\n",
            "\n",
            "Archivo: audio06\n",
            "Texto transcrito:\n",
            " Las fábulas de Esopo, grabado para LibriVox.org por Alejandro González Calderón. Fábula número 66, El lobo y el asno. Un lobo fue elegido rey entre sus congéneres y decretó una ley ordenando que lo que cada uno capturase en la casa, lo pusiera en común y lo repartiese por partes iguales entre todos. de esta manera ya no tendrían los lobos que devorarse unos a otros en épocas de hambre pero en eso le escuchó un asno que estaba por ahí cerca y moviendo sus orejas le dijo magnífica idea ha brotado de tu corazón pero ¿por qué has escondido todo tu botín en tu cueva? lleválo a la comunidad y repártelo también como lo has decretado el lobo descubierto y confundido derogó su ley Si alguna vez llegas a tener poder de legislar, sé el primero en cumplir tus propias leyes Fin de la fábula, esta grabación es de dominio público\n",
            "\n",
            "\n",
            "Archivo: audio14\n",
            "Texto transcrito:\n",
            " Las fábulas de Esopo. Grabado para LibriVox.org por El Ochito. Fábula número 74. El lobo y el cabrito encerrado. Protegido por la seguridad del corral de una casa, un cabrito vio pasar a un lobo y comenzó a insultarle burlándose ampliamente de él. El lobo serenamente le replicó. Infeliz, sé que no eres tú quien me está insultando sino el sitio en que te encuentras Muy a menudo no es el valor sino la ocasión y el lugar quienes proveen el enfrentamiento arrogante ante los poderosos Fin de la fábula Esta grabación es del dominio público Gracias.\n",
            "\n",
            "\n",
            "Archivo: audio22\n",
            "Texto transcrito:\n",
            " Las fábulas de Esopo. Grabado para LibriVox.org por El Ochito. Fábula número 82. El perro y la almeja. Un perro de esos acostumbrados a comer huevos, al ver una almeja, no lo pensó dos veces y, creyendo que se trataba de un huevo, se la tragó inmediatamente. Desgarradas luego sus entrañas, se sintió muy mal y se dijo, bien merecido lo tengo, por creer que todo lo que veo redondo son huevos. Nunca tomes un asunto sin antes reflexionar, para no entrar luego en extrañas dificultades. Fin de la fábula. Esta grabación es del dominio público.\n",
            "\n",
            "\n",
            "Archivo: audio24\n",
            "Texto transcrito:\n",
            " Las fábulas de Sopo. Grabado para LibriVox.org por Karen Savage. Fábula número 84. El perro y el reflejo en el río. Badiaba un perro un río, llevando en su hocico un sabroso pedazo de carne. Vio su propio reflejo en el agua del río, y creyó que aquel reflejo era en realidad otro perro que llevaba un trozo de carne mayor que el suyo. Y deseando adueñarse del pedazo ajeno, soltó el suyo para arrebatar el trozo a su supuesto compadre, pero el resultado fue que se quedó sin el propio y sin el ajeno. Este, porque no existía, sólo era un reflejo, y el otro, el verdadero, porque se lo llevó a la corriente. Nunca codicies el bien ajeno, pues puedes perder lo que ya has adquirido con tu esfuerzo. Fin de fábula Esta grabación es de dominio público\n",
            "\n",
            "\n",
            "Archivo: audio25\n",
            "Texto transcrito:\n",
            " Las fábulas de Esopo, grabado para LibreVox.org, fábula número 85, El perro y el carnicero. Penetró un perro en una carnicería y notando que el carnicero estaba muy ocupado con sus clientes, cogió un trozo de carne y se lió corriendo. Se volvió el carnicero y viéndole huir, y sin poder hacer nada, exclamó. Oye amigo, allí donde te encuentre no dejaré de mirarte. No esperes a que suceda un accidente para pensar en cómo evitarlo. Fin de fábula Esta grabación es de dominio público. Gracias.\n",
            "\n",
            "\n",
            "Archivo: audio26\n",
            "Texto transcrito:\n",
            " Las Fábulas de Esopo, grabado para LibriVox.org por El Ochito. Fábula número 86. El perro con campanilla. Había un perro que acostumbraba a morder sin razón. Le puso su amo una campanilla para advertirle a la gente de su presencia cercana, y el can, sonando la campanilla, se fue a la plaza pública a presumir. Mas una sabia perra, ya avanzada de años, le dijo ¿De qué presumes tanto, amigo? Sé que no llevas esa campanilla por tus grandes virtudes, sino para anunciar tu maldad oculta Los halagos que se hacen a sí mismo, los fanfarrones, sólo delatan sus mayores defectos Fin de la fábula Esta grabación es del dominio público Gracias por ver el video.\n",
            "\n",
            "\n",
            "Archivo: audio27\n",
            "Texto transcrito:\n",
            " Las fábulas de Esopo. Grabado para LibriVox.org por El Ochito. Fábula número 87. El perro que perseguía al león. Un perro de casa se encontró con un león y partió en su persecución. Pero el león se volvió rugiendo, y el perro, todo atemorizado, retrocedió rápidamente por el mismo camino. Le vio una zorra y le dijo, ¡Perro infeliz! Primero perseguías al león y ya ni siquiera soportas sus surgidos. Cuando entres a una empresa, mantente siempre listo a afrontar imprevistos que no te imaginabas. Fin de la fábula. Esta grabación es del dominio público. Gracias.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#importar librerías y configurar CUDA\n",
        "import torch\n",
        "from transformers import pipeline, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "import librosa\n",
        "\n",
        "# Configurar dispositivo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "\n",
        "#Modelo Whisper en Hugging face\n",
        "model_id = \"openai/whisper-large-v3-turbo\"\n",
        "\n",
        "# Cargar modelo y procesador\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "modelo = AutoModelForSpeechSeq2Seq.from_pretrained(model_id).to(device)\n",
        "\n",
        "# Inicializar pipeline\n",
        "pipe = pipeline(\"automatic-speech-recognition\",\n",
        "                model=modelo,\n",
        "                tokenizer=processor.tokenizer,\n",
        "                feature_extractor=processor.feature_extractor,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device=device)\n",
        "\n",
        "#Transcribir el audio y guardar los resultados\n",
        "# Diccionario para almacenar transcripciones\n",
        "transcripciones = {}\n",
        "\n",
        "# Procesar cada archivo de audio\n",
        "for key, audio_path in audio_files.items():\n",
        "    print(f\"Procesando {key}...\")\n",
        "    result = pipe(audio_path,\n",
        "                  return_timestamps=True,   # Para audios mayores a 30 segs.\n",
        "                  generate_kwargs={\"language\": \"es\"})\n",
        "    transcripciones[key] = result[\"text\"]\n",
        "\n",
        "# Mostrar los primeros resultados\n",
        "for key, value in transcripciones.items():\n",
        "    print(f\"\\nArchivo: {key}\")\n",
        "    print(f\"Texto transcrito:\\n{value}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM0D83j8EWiN"
      },
      "source": [
        "# **Ejercicio 2b:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiFG5q88EYHU"
      },
      "source": [
        "* #### **Eliminar el inicio y final comunes de los textos extraídos de cada fábula.**\n",
        "\n",
        "* #### **Sugerencia:** Pueden guardar esta información en un archivo tipo JSON, para que al estar probando diferentes opciones en los ejercicios siguientes, puedan recuperar rápidamente la información de cada video/fábula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkbeTmeon_RP",
        "outputId": "e2363326-e7c1-44f0-def4-72d64578e3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔹 **Antes (audio01):**\n",
            " Las Fábulas de Esopo, grabado para LibriVox.org por Paulino, www.paulino.info. Fábula número 61. El Lobo y el Cordero en el Templo. Dándose cuenta de que era perseguido por un lobo, un pequeño corderito decidió refugiarse en un templo cercano. Lo llamó lobo y le dijo que si el sacrificador lo encontraba allí adentro, lo enmolaría a su dios. Mejor así, replicó el cordero, prefiero ser víctima para un dios a tener que perecer en tus colmillos. Si sin remedio vamos a ser sacrificados, más nos vale que sea con el mayor honor. Fin de la fábula Esta es una grabación del dominio público.\n",
            "\n",
            "✅ **Después (audio01):**\n",
            "El Lobo y el Cordero en el Templo. Dándose cuenta de que era perseguido por un lobo, un pequeño corderito decidió refugiarse en un templo cercano. Lo llamó lobo y le dijo que si el sacrificador lo encontraba allí adentro, lo enmolaría a su dios. Mejor así, replicó el cordero, prefiero ser víctima para un dios a tener que perecer en tus colmillos. Si sin remedio vamos a ser sacrificados, más nos vale que sea con el mayor honor.\n",
            "\n",
            "🎯 **Transcripciones limpias guardadas en /content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_recortadas.json**\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Expresión regular mejorada para eliminar todo lo anterior al número de la fábula\n",
        "inicio_regex = r\"^.*?[Ff]ábula número \\d+[.,]?\\s*\"\n",
        "\n",
        "# Expresión regular para eliminar desde \"Fin de la fábula\" hasta el final\n",
        "final_regex = r\"\\s*Fin de la fábula.*?$\"\n",
        "\n",
        "# Diccionario para almacenar transcripciones limpias\n",
        "transcripciones_recortadas = {}\n",
        "\n",
        "# Limpiar cada transcripción eliminando el inicio y el final\n",
        "for key, texto in transcripciones.items():\n",
        "    texto_limpio = re.sub(inicio_regex, \"\", texto)  # Eliminar inicio\n",
        "    texto_limpio = re.sub(final_regex, \"\", texto_limpio)  # Eliminar final\n",
        "    transcripciones_recortadas[key] = texto_limpio.strip()\n",
        "\n",
        "# Mostrar el antes y después de la primera transcripción como ejemplo\n",
        "primer_audio = list(transcripciones.keys())[0]  # Obtener primera clave de audio\n",
        "print(f\"\\n🔹 **Antes ({primer_audio}):**\\n{transcripciones[primer_audio]}\")\n",
        "print(f\"\\n✅ **Después ({primer_audio}):**\\n{transcripciones_recortadas[primer_audio]}\")\n",
        "\n",
        "# Guardar transcripciones limpias en un archivo JSON\n",
        "archivo_json = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_recortadas.json\"\n",
        "with open(archivo_json, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(transcripciones_recortadas, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"\\n🎯 **Transcripciones limpias guardadas en {archivo_json}**\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PKaB_Ge0Shc"
      },
      "source": [
        "# **Ejercicio 3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNrqcQFe0VWR"
      },
      "source": [
        "* #### **Apliquen el proceso de limpieza que consideren adecuado.**\n",
        "\n",
        "* #### **Justifiquen los pasos de limpieza utilizados. Tomen en cuenta que el texto extraído de cada fábula es relativamente pequeño.**\n",
        "\n",
        "* #### **En caso de que decidan no aplicar esta etapa de limpieza, deberán justificarlo.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS91CHdJ2nD2"
      },
      "source": [
        "Tomando en cuenta que el texto de las fábulas es corto, se busca un equilibrio entre limpieza y preservación del contenido esencial.\n",
        "Se va a considerar:\n",
        "\n",
        "\n",
        "*   Eliminar caracteres especiales innecesarios para mejorar la legibilidad y evitar ruido en análisis posteriores.\n",
        "*   Eliminar espacios dobles y saltos de línea para evitar problemas en el procesamiento y mejorar la estructura del texto.\n",
        "*   Eliminar algunas stopwords, como las fábulas tienen un lenguaje narrativo corto quitar algunas stopwords como \"pero\", \"porque\" o \"si\" podría afectar el significado.Solo se quitaran stopwords que no alteren la narrativa como \"el\", \"la\", etc.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "No se eliminarán:\n",
        "\n",
        "\n",
        "*   Palabras con baja frecuencia porque al ser un texto corto, muchas palabras aparecerán pocas veces y eliminarlas podría afectar el mensaje (ej: animales principales de la fábula).\n",
        "*   Palabras con longitud muy corta porque algunas afectarían el sentido de frases clave (ej: si, no).\n",
        "*  Toda la puntuación porque en textos narrativos se perdería la estructura y claridad si se eliminan comas y puntos.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "pqwiCCdpq8D_",
        "outputId": "36a345bb-2a90-43f9-e677-03444bc08f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'transcripciones_recortadas' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3f6c505f2d60>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Diccionario de transcripciones limpias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtranscripciones_limpias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlimpiar_texto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexto\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranscripciones_recortadas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Mostrar antes y después de la primera transcripción\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transcripciones_recortadas' is not defined"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar lista de stopwords en español\n",
        "nltk.download('stopwords')\n",
        "stop_words_default = set(stopwords.words('spanish'))\n",
        "\n",
        "# Lista personalizada de stopwords que sí eliminaremos (filtradas según contexto)\n",
        "stop_words_filtradas = {\n",
        "    \"de\", \"la\", \"las\", \"los\", \"el\", \"un\", \"una\", \"unos\", \"unas\",\n",
        "    \"en\", \"es\", \"al\", \"del\", \"con\", \"por\", \"para\", \"se\", \"que\"\n",
        "}\n",
        "\n",
        "# Expresión regular para eliminar caracteres especiales innecesarios\n",
        "special_chars_regex = r\"[^a-zA-ZáéíóúüñÁÉÍÓÚÜÑ0-9\\s]\"\n",
        "\n",
        "# Función para limpiar cada texto\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()  # Convertir a minúsculas\n",
        "    texto = re.sub(special_chars_regex, \"\", texto)  # Eliminar caracteres especiales\n",
        "    texto = re.sub(r\"\\s+\", \" \", texto).strip()  # Eliminar espacios dobles y saltos de línea\n",
        "\n",
        "    # Eliminar solo las stopwords de nuestra lista personalizada\n",
        "    palabras = texto.split()\n",
        "    texto_limpio = \" \".join([palabra for palabra in palabras if palabra not in stop_words_filtradas])\n",
        "\n",
        "    return texto_limpio\n",
        "\n",
        "# Diccionario de transcripciones limpias\n",
        "transcripciones_limpias = {key: limpiar_texto(texto) for key, texto in transcripciones_recortadas.items()}\n",
        "\n",
        "# Mostrar antes y después de la primera transcripción\n",
        "primer_audio = list(transcripciones_recortadas.keys())[0]\n",
        "print(f\"\\n🔹 **Antes ({primer_audio}):**\\n{transcripciones_recortadas[primer_audio]}\")\n",
        "print(f\"\\n✅ **Después ({primer_audio}):**\\n{transcripciones_limpias[primer_audio]}\")\n",
        "\n",
        "# Guardar transcripciones limpias en JSON\n",
        "archivo_json = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_limpias.json\"\n",
        "with open(archivo_json, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(transcripciones_limpias, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"\\n🎯 **Transcripciones limpias guardadas en {archivo_json}**\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2ywrmsMP_EF"
      },
      "source": [
        "# **Ejercicio 4:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xFpnt0A0Ub7",
        "outputId": "fd6919ca-b53a-45b6-cbf6-50ad607603fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ **Pesos de palabras guardados en /content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/palabras_con_pesos.json**\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "import nltk\n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Descargar recursos de NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 📌 **1. Cargar las fábulas limpias**\n",
        "archivo_json = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_limpias.json\"\n",
        "with open(archivo_json, \"r\", encoding=\"utf-8\") as f:\n",
        "    fabulas_limpias = json.load(f)\n",
        "\n",
        "# 📌 **2. Aplicar LDA individualmente a cada fábula**\n",
        "num_palabras = 20  # Número de palabras clave por fábula\n",
        "pesos_palabras_fabulas = {}\n",
        "\n",
        "for key, texto in fabulas_limpias.items():\n",
        "    # Tokenizar la fábula individualmente\n",
        "    tokens_fabula = word_tokenize(texto)\n",
        "\n",
        "    # Crear diccionario y corpus solo para esta fábula\n",
        "    diccionario = Dictionary([tokens_fabula])\n",
        "    corpus = [diccionario.doc2bow(tokens_fabula)]\n",
        "\n",
        "    # Aplicar LDA SOLO a la fábula actual\n",
        "    modelo_lda = LdaModel(corpus, num_topics=1, id2word=diccionario, passes=10)\n",
        "\n",
        "    # Extraer palabras clave y sus pesos del único tópico generado\n",
        "    palabras_con_pesos = modelo_lda.show_topic(0, topn=num_palabras)\n",
        "\n",
        "    # Convertir pesos a `float` para evitar errores de serialización\n",
        "    pesos_palabras_fabulas[key] = {palabra: float(peso) for palabra, peso in palabras_con_pesos}\n",
        "\n",
        "# 📌 **3. Guardar resultados en JSON**\n",
        "archivo_salida = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/palabras_con_pesos.json\"\n",
        "with open(archivo_salida, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(pesos_palabras_fabulas, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"\\n✅ **Pesos de palabras guardados en {archivo_salida}**\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blrrs1sWwkSx"
      },
      "source": [
        "# **Ejercicio 5a y 5b:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWvzQ-aNwsVk"
      },
      "source": [
        "* #### **5a: Mediante el LLM que hayan seleccionado, generar un único enunciado que describa o resuma cada fábula.**\n",
        "\n",
        "* #### **5b: Mediante el LLM que hayan seleccionado, generar tres posibles enunciados diferentes relacionados con la historia de la fábula.**\n",
        "\n",
        "* #### **Sugerencia:** En realidad los dos incisos a y b se pueden obtener con un solo prompt que solicite la información y el formato correspondiente para cada una de estas partes. Por ejemplo, para cada fábula la salida puede ser un primer enunciado genérico que resume o describe dicha temática; seguido de tres enunciados, cada uno hablando sobre una situación o parte diferente de la fábula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BXeGlgwEWGvc"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import gc\n",
        "from transformers import pipeline\n",
        "\n",
        "# 🔹 Liberar memoria antes de la ejecución (Optimización en Colab)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 📌 **1. Cargar palabras clave y sus pesos extraídas con LDA**\n",
        "archivo_json = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/palabras_con_pesos.json\"\n",
        "with open(archivo_json, \"r\", encoding=\"utf-8\") as f:\n",
        "    pesos_palabras_fabulas = json.load(f)\n",
        "\n",
        "# 📌 **2. Selección del modelo LLM en Hugging Face**\n",
        "modelo_huggingface = \"microsoft/Phi-2\"  # Modelo ligero y eficiente\n",
        "llm_pipeline = pipeline(\"text-generation\", model=modelo_huggingface, torch_dtype=torch.float16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 📌 **3. Generación de resúmenes y subtemas**\n",
        "resultados = {}\n",
        "\n",
        "for key, palabras_con_pesos in pesos_palabras_fabulas.items():\n",
        "    # **Organizar palabras según su peso**\n",
        "    palabras_ordenadas = sorted(palabras_con_pesos.items(), key=lambda x: x[1], reverse=True)\n",
        "    palabras_clave = [palabra for palabra, peso in palabras_ordenadas]\n",
        "\n",
        "    # 📌 **4. Mejorar el prompt para el resumen**\n",
        "    prompt_resumen = f\"\"\"\n",
        "Eres un narrador experto en literatura y fábulas.\n",
        "Tu tarea es escribir un **único enunciado** que resuma la historia basada en las siguientes palabras clave.\n",
        "Las palabras más importantes están al inicio y deben ser el centro de la historia.\n",
        "Evita definiciones, explicaciones o contenido ajeno a la fábula.\n",
        "Escribe una historia clara, directa y con sentido.\n",
        "\n",
        "Ejemplo de salida:\n",
        "📖 *El lobo persiguió al cordero hasta el templo, pero fue detenido por los dioses, que castigaron su ferocidad.*\n",
        "\n",
        "Ahora genera un resumen basado en estas palabras clave:\n",
        "\n",
        "Palabras clave con peso: {', '.join(palabras_clave)}\n",
        "\n",
        "Resumen:\n",
        "\"\"\"\n",
        "\n",
        "    # 📌 **5. Mejorar el prompt para los subtemas**\n",
        "    prompt_subtemas = f\"\"\"\n",
        "Usando las siguientes palabras clave, escribe **tres enunciados** diferentes que se relacionen con la fábula.\n",
        "Cada enunciado debe explorar un aspecto distinto de la historia.\n",
        "No repitas el resumen, cada subtema debe agregar una perspectiva nueva.\n",
        "\n",
        "Ejemplo de salida:\n",
        "🔹 *La justicia divina castiga a quienes actúan con maldad.*\n",
        "🔹 *El lobo representa la agresividad y el peligro de dejarse llevar por los instintos.*\n",
        "🔹 *El cordero simboliza la inocencia y la vulnerabilidad en un mundo cruel.*\n",
        "\n",
        "Ahora genera tres enunciados basados en estas palabras clave:\n",
        "\n",
        "Palabras clave con peso: {', '.join(palabras_clave)}\n",
        "\n",
        "Subtemas:\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "\"\"\"\n",
        "\n",
        "    # 📌 **6. Generar el resumen y los subtemas con Hugging Face LLM**\n",
        "    respuesta_resumen = llm_pipeline(prompt_resumen, max_length=50, do_sample=True, temperature=0.8)[0][\"generated_text\"].replace(prompt_resumen, \"\").strip()\n",
        "    respuesta_subtemas = llm_pipeline(prompt_subtemas, max_length=100, do_sample=True, temperature=0.8)[0][\"generated_text\"].replace(prompt_subtemas, \"\").strip().split(\"\\n\")\n",
        "\n",
        "    # 📌 **7. Filtrar respuestas irrelevantes y asegurar estructura**\n",
        "    respuesta_subtemas = [s.strip() for s in respuesta_subtemas if len(s.strip()) > 5 and not s.strip().isdigit()]\n",
        "    while len(respuesta_subtemas) < 3:\n",
        "        respuesta_subtemas.append(\"N/A\")  # Evita listas incompletas\n",
        "\n",
        "    # 📌 **8. Guardar resultados**\n",
        "    resultados[key] = {\n",
        "        \"resumen\": respuesta_resumen,\n",
        "        \"subtemas\": respuesta_subtemas[:3]  # Limita a 3 subtemas\n",
        "    }\n",
        "\n",
        "# 📌 **9. Guardar resultados en JSON**\n",
        "archivo_salida = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_resumen_subtemas.json\"\n",
        "with open(archivo_salida, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(resultados, f, ensure_ascii=False, indent=4, sort_keys=True)\n",
        "\n",
        "print(f\"\\n✅ **Resúmenes y subtemas generados y guardados en {archivo_salida}**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868,
          "referenced_widgets": [
            "4a5a6e0c5dda4b4e954a03548f8501c8",
            "d118fb471f934598bee403f3fa3d01d9",
            "5691e23557f34a7eb2597fd012539d01",
            "e00518f0e4cb4a5a9e9bf06c0ed15e98",
            "06d5977410de41cba0acfd85ee5e16f4",
            "bdee4d7eb2454b07a8092d02e3b420f4",
            "b79efc35283c4155a5d2f87b148e8c6b",
            "08c6a87d2a614d63bee11cfc14231bef",
            "20d908d2b690478ba82b3840bac960e1",
            "bbfa573d052b428ba7ba9505f5c1ce98",
            "a01f66c2f0814f54b33b9a2b5739968a"
          ]
        },
        "id": "7s7tzj2qxrA7",
        "outputId": "ad4f04b5-bc4e-4c3d-8f2f-219162f97370"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a5a6e0c5dda4b4e954a03548f8501c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ **Resúmenes y subtemas generados y guardados en /content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_resumen_subtemas.json**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868,
          "referenced_widgets": [
            "a26b594632b04457a6daf8d153a4b5d8",
            "8c079b5e63ee4bda9fdffe76556def84",
            "da13d23cf9154e9dbab742f56bfcd0fb",
            "a758f7fa52034ca0acaec04f59a7ff89",
            "35b9523adb3a4f66b12c464158164a4f",
            "ab8d0956791d460e845f1593675377fc",
            "dc77bbe4e5364974b09b76cf45b2d06b",
            "51a6dbef47b14aadb49ef39520a6d9b5",
            "f0e4f32da38a4407a6c7b8b09e826b16",
            "9150a30d43cb4f3787a182b601cbbe60",
            "d73cbb5da23447f6b9771758e2de858b"
          ]
        },
        "id": "Q9UkVPxM0Xii",
        "outputId": "4f749d1c-e67a-4b9f-b1f5-78d901a79278"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a26b594632b04457a6daf8d153a4b5d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 **Resúmenes y subtemas guardados en /content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_resumen_subtemas.json**\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "# Cargar palabras clave de cada fábula\n",
        "archivo_json = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/palabras_clave.json\"\n",
        "with open(archivo_json, \"r\", encoding=\"utf-8\") as f:\n",
        "    palabras_clave_fabulas = json.load(f)\n",
        "\n",
        "# Selección del modelo LLM en Hugging Face\n",
        "modelo_huggingface = \"microsoft/Phi-2\"  # Lightweight alternative\n",
        "#\"mistralai/Mistral-7B-Instruct\", \"meta-llama/Llama-3-8B\" , \"tiiuae/falcon-7b-instruct\", \"bigscience/bloomz-7b\"\n",
        "\n",
        "# Inicializar el pipeline de generación de texto\n",
        "llm_pipeline = pipeline(\"text-generation\", model=modelo_huggingface, torch_dtype=torch.float16)\n",
        "\n",
        "# Generación de resúmenes y subtemas\n",
        "resultados = {}\n",
        "\n",
        "for key, palabras in palabras_clave_fabulas.items():\n",
        "    prompt_resumen = f\"Usando las siguientes palabras clave, genera un único enunciado que describa la fábula:\\nPalabras clave: {', '.join(palabras)}\"\n",
        "    prompt_subtemas = f\"Usando solo las siguientes palabras clave, genera cinco posibles subtemas diferentes para esta fábula.Responde únicamente con una lista, sin introducciones ni explicaciones. Palabras clave: {', '.join(palabras)}\"\n",
        "\n",
        "    # Generación con Hugging Face LLM\n",
        "    respuesta_resumen = llm_pipeline(prompt_resumen, max_length=30, do_sample=True)[0][\"generated_text\"]\n",
        "    respuesta_subtemas = llm_pipeline(prompt_subtemas, max_length=10, do_sample=True)[0][\"generated_text\"].split(\"\\n\")\n",
        "\n",
        "    # Guardar resultados\n",
        "    resultados[key] = {\n",
        "        \"resumen\": respuesta_resumen,\n",
        "        \"subtemas\": respuesta_subtemas\n",
        "    }\n",
        "\n",
        "# Guardar resultados en JSON\n",
        "archivo_salida = \"/content/drive/MyDrive/Estudios/MNA/NLP/Audio2Text/fabulas_resumen_subtemas.json\"\n",
        "with open(archivo_salida, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(resultados, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"\\n🎯 **Resúmenes y subtemas guardados en {archivo_salida}**\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-dZSFJz9cK"
      },
      "source": [
        "# **Ejercicio 6:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w3usdaC0BCj"
      },
      "source": [
        "* #### **Incluyan sus conclusiones de la actividad audio-a-texto:**\n",
        "\n",
        "\n",
        "\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtB5Q3m41YQ0"
      },
      "source": [
        "# **Fin de la actividad LDA y LMM: audio-a-texto**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a5a6e0c5dda4b4e954a03548f8501c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d118fb471f934598bee403f3fa3d01d9",
              "IPY_MODEL_5691e23557f34a7eb2597fd012539d01",
              "IPY_MODEL_e00518f0e4cb4a5a9e9bf06c0ed15e98"
            ],
            "layout": "IPY_MODEL_06d5977410de41cba0acfd85ee5e16f4"
          }
        },
        "d118fb471f934598bee403f3fa3d01d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdee4d7eb2454b07a8092d02e3b420f4",
            "placeholder": "​",
            "style": "IPY_MODEL_b79efc35283c4155a5d2f87b148e8c6b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5691e23557f34a7eb2597fd012539d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08c6a87d2a614d63bee11cfc14231bef",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20d908d2b690478ba82b3840bac960e1",
            "value": 2
          }
        },
        "e00518f0e4cb4a5a9e9bf06c0ed15e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbfa573d052b428ba7ba9505f5c1ce98",
            "placeholder": "​",
            "style": "IPY_MODEL_a01f66c2f0814f54b33b9a2b5739968a",
            "value": " 2/2 [00:00&lt;00:00,  2.38it/s]"
          }
        },
        "06d5977410de41cba0acfd85ee5e16f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdee4d7eb2454b07a8092d02e3b420f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b79efc35283c4155a5d2f87b148e8c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08c6a87d2a614d63bee11cfc14231bef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d908d2b690478ba82b3840bac960e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbfa573d052b428ba7ba9505f5c1ce98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a01f66c2f0814f54b33b9a2b5739968a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a26b594632b04457a6daf8d153a4b5d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c079b5e63ee4bda9fdffe76556def84",
              "IPY_MODEL_da13d23cf9154e9dbab742f56bfcd0fb",
              "IPY_MODEL_a758f7fa52034ca0acaec04f59a7ff89"
            ],
            "layout": "IPY_MODEL_35b9523adb3a4f66b12c464158164a4f"
          }
        },
        "8c079b5e63ee4bda9fdffe76556def84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab8d0956791d460e845f1593675377fc",
            "placeholder": "​",
            "style": "IPY_MODEL_dc77bbe4e5364974b09b76cf45b2d06b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "da13d23cf9154e9dbab742f56bfcd0fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51a6dbef47b14aadb49ef39520a6d9b5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0e4f32da38a4407a6c7b8b09e826b16",
            "value": 2
          }
        },
        "a758f7fa52034ca0acaec04f59a7ff89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9150a30d43cb4f3787a182b601cbbe60",
            "placeholder": "​",
            "style": "IPY_MODEL_d73cbb5da23447f6b9771758e2de858b",
            "value": " 2/2 [00:00&lt;00:00,  1.29it/s]"
          }
        },
        "35b9523adb3a4f66b12c464158164a4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab8d0956791d460e845f1593675377fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc77bbe4e5364974b09b76cf45b2d06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51a6dbef47b14aadb49ef39520a6d9b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e4f32da38a4407a6c7b8b09e826b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9150a30d43cb4f3787a182b601cbbe60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d73cbb5da23447f6b9771758e2de858b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}